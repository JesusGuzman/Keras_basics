{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guardar modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el siguiente ejercicio construiremos un modelo basico, lo entrenaremos con datos de minst para posteriormente guardar los pesos del modelo y poder utilizarlos mas adelante"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargamos las librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from keras.utils import np_utils\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "np.random.seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Definimos nuestros Hyperparametros\n",
    "batch_size = 128 \n",
    "nb_classes = 10\n",
    "nb_epoch = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparamos nuestros datos\n",
    "Para este ejercicio utilizamos los datos de mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# Cargamos nuestros datos y los dividimos en datos de entrenamiento y prueba\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train = X_train.reshape(60000, 784)\n",
    "X_test = X_test.reshape(10000, 784)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "# Convertimos los vectores a matrices binarias\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construcción del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                5130      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(784,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "#Mostramos la arquitectura del modelo\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compilación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hadoop/anaconda2/envs/flow/lib/python2.7/site-packages/keras/models.py:942: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 14s 230us/step - loss: 0.2429 - acc: 0.9253 - val_loss: 0.1150 - val_acc: 0.9637\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 13s 224us/step - loss: 0.1008 - acc: 0.9694 - val_loss: 0.0823 - val_acc: 0.9770\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 14s 227us/step - loss: 0.0758 - acc: 0.9772 - val_loss: 0.0793 - val_acc: 0.9764\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 14s 230us/step - loss: 0.0603 - acc: 0.9815 - val_loss: 0.0923 - val_acc: 0.9745\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 14s 231us/step - loss: 0.0511 - acc: 0.9857 - val_loss: 0.0790 - val_acc: 0.9793\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 14s 232us/step - loss: 0.0430 - acc: 0.9870 - val_loss: 0.0773 - val_acc: 0.9817\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 14s 236us/step - loss: 0.0389 - acc: 0.9890 - val_loss: 0.0745 - val_acc: 0.9825\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 14s 235us/step - loss: 0.0338 - acc: 0.9899 - val_loss: 0.0845 - val_acc: 0.9810\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 14s 235us/step - loss: 0.0315 - acc: 0.9906 - val_loss: 0.0902 - val_acc: 0.9808\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 13s 220us/step - loss: 0.0294 - acc: 0.9919 - val_loss: 0.0864 - val_acc: 0.9836\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 14s 226us/step - loss: 0.0241 - acc: 0.9929 - val_loss: 0.0936 - val_acc: 0.9821\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 13s 224us/step - loss: 0.0245 - acc: 0.9930 - val_loss: 0.1016 - val_acc: 0.9813\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 14s 229us/step - loss: 0.0223 - acc: 0.9937 - val_loss: 0.0881 - val_acc: 0.9836\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 14s 234us/step - loss: 0.0229 - acc: 0.9939 - val_loss: 0.0979 - val_acc: 0.9844\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 14s 240us/step - loss: 0.0222 - acc: 0.9940 - val_loss: 0.0917 - val_acc: 0.9842\n",
      "Epoch 16/20\n",
      "35328/60000 [================>.............] - ETA: 5s - loss: 0.0177 - acc: 0.9949"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, Y_train,\n",
    "                    batch_size=batch_size, nb_epoch=nb_epoch,\n",
    "                    verbose=1, validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluación del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guardar el modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despues del entrenamiento, nuestro modelo ya tiene los pesos y los sesgos optimos para la inferencia de resultados no vistos antes, podemos guardar estos parametros en un archivo h5 para poder utilizarlos mas adelante, la forma de guardar estos parametros en un archivo es de la siguiente forma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('mnist-mpl.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "este archivo se guarda en la carpeta, listo para ser utilizado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
